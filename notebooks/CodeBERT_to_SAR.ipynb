{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CodeBERT to SAR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cc39e0fbde34ba5a1123403dad1addc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0378b7a748a8487fa88d707f0361dae7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35726ef88a9e44c4a9ae3e3a4486ab6f",
              "IPY_MODEL_e97a9174ba2941f58397b4c904f3a56c"
            ]
          }
        },
        "0378b7a748a8487fa88d707f0361dae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35726ef88a9e44c4a9ae3e3a4486ab6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ada11a22845d46a2b8d2c4b2d897547f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9da3b3f3613416cb491daeaf4faca70"
          }
        },
        "e97a9174ba2941f58397b4c904f3a56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a2642302ad34ecea6a1e17b8007e124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 498/498 [00:00&lt;00:00, 844B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5584fa2849674d22af1d55801d124675"
          }
        },
        "ada11a22845d46a2b8d2c4b2d897547f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9da3b3f3613416cb491daeaf4faca70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a2642302ad34ecea6a1e17b8007e124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5584fa2849674d22af1d55801d124675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fd7601f47a842ada775539fce2594bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4efc45da339a4e72a939b80a8ec3bb81",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cf6519034e6349b3b7553dd7c4389e2c",
              "IPY_MODEL_20367294f4294edc937b79534b923a8c"
            ]
          }
        },
        "4efc45da339a4e72a939b80a8ec3bb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf6519034e6349b3b7553dd7c4389e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7983c2da71734663862fa92597a17f6b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0bec18ca9674822b75f8b2935959071"
          }
        },
        "20367294f4294edc937b79534b923a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57d8249f7822454d98ab4018cdad850f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 6.62MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_154b38827bd04bb097df5a24734fb86e"
          }
        },
        "7983c2da71734663862fa92597a17f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0bec18ca9674822b75f8b2935959071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57d8249f7822454d98ab4018cdad850f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "154b38827bd04bb097df5a24734fb86e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6711a825a8754b0e8214123be210db12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_12893659767b4657bd3833872ada8f95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdd02fd616b94263861e50cdf82c6119",
              "IPY_MODEL_75c53f42c79e4732b765c440b4046e98"
            ]
          }
        },
        "12893659767b4657bd3833872ada8f95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdd02fd616b94263861e50cdf82c6119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bb57f46dba94105b232d658f805eccb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9818bf784b81439391603a96d05b84d6"
          }
        },
        "75c53f42c79e4732b765c440b4046e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3bdaf5d29714a709bd7298cb9fdae48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.25MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50756a8b37ab44dab0faa4a197465c29"
          }
        },
        "6bb57f46dba94105b232d658f805eccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9818bf784b81439391603a96d05b84d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3bdaf5d29714a709bd7298cb9fdae48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50756a8b37ab44dab0faa4a197465c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec9c736619de47288bb25b3b59ad6f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cd73be9435a54dd3b860e666263569a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23a529bb4303446d8d054b7ca13c795a",
              "IPY_MODEL_2cbe9b8240df4be3961443f09589cdd6"
            ]
          }
        },
        "cd73be9435a54dd3b860e666263569a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23a529bb4303446d8d054b7ca13c795a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5d1463aef32544febc58e0ccf69a8af7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c67f539f22634bf1936f59705a61a417"
          }
        },
        "2cbe9b8240df4be3961443f09589cdd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6bba3f9de8344c5884248a8c75675ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 631B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c7e1315c1c74884bfad2191bd3a90c0"
          }
        },
        "5d1463aef32544febc58e0ccf69a8af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c67f539f22634bf1936f59705a61a417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6bba3f9de8344c5884248a8c75675ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c7e1315c1c74884bfad2191bd3a90c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43e6ac5249644e0c894ec88f210129c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cc00f3d4f9894db2878927639503977e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46a30bfbb5ee425aa6429d82624e3702",
              "IPY_MODEL_a61d7a9f20fa497fab648c40084c012e"
            ]
          }
        },
        "cc00f3d4f9894db2878927639503977e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46a30bfbb5ee425aa6429d82624e3702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa002f87cb7f4cd09ae3062feb733688",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 25,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 25,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2c2afb44fff4b8393f8eb3014498844"
          }
        },
        "a61d7a9f20fa497fab648c40084c012e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ce8f5553ad794670979d24f392ebd3a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25.0/25.0 [00:00&lt;00:00, 187B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63f0062defe54bbd9089b98152fd1f41"
          }
        },
        "aa002f87cb7f4cd09ae3062feb733688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2c2afb44fff4b8393f8eb3014498844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce8f5553ad794670979d24f392ebd3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63f0062defe54bbd9089b98152fd1f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5133eec1fd52496ea913a7d2fa82cc42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a8efcb940ef14db1bacb4aa2cc2ca923",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85bcce111b0e495a8338413081267d3d",
              "IPY_MODEL_353b99d6ae354d0c97a0197475840a94"
            ]
          }
        },
        "a8efcb940ef14db1bacb4aa2cc2ca923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85bcce111b0e495a8338413081267d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55a3be037c5041d1ba8f1a5ac7e5442d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498627950,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498627950,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_844d1ed354fd44089f5fcb2062a584f9"
          }
        },
        "353b99d6ae354d0c97a0197475840a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0dcd14c184f4a36a8dd728b2aa1d0c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 499M/499M [00:09&lt;00:00, 51.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10be7dca34284199835ee54c778a6678"
          }
        },
        "55a3be037c5041d1ba8f1a5ac7e5442d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "844d1ed354fd44089f5fcb2062a584f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0dcd14c184f4a36a8dd728b2aa1d0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10be7dca34284199835ee54c778a6678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masum06/Text2App/blob/master/notebooks/CodeBERT_to_SAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F864UIM1YUEq"
      },
      "source": [
        "Code borrowed from: https://github.com/microsoft/CodeXGLUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBG3u_eYQHKY",
        "outputId": "6e478802-86d2-46e7-918b-58ba2adac4e8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 17.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=55a8d06deebcf1e36d0927dab0b070812313b20fc0803497d515ba402bcc6057\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHNqpkh3Q2MG",
        "outputId": "15941214-e10f-470a-8fe4-b9a5bffdfbe8"
      },
      "source": [
        "!git clone https://github.com/Masum06/Text2App.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Text2App'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 414 (delta 9), reused 16 (delta 9), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (414/414), 198.26 MiB | 45.59 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc12sfRZSmxT",
        "outputId": "3a6b868b-6aff-434e-bac7-9e2d5985b924"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  7 10:58:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLNH4uJG7rI"
      },
      "source": [
        "# Text2App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jd61C2XOiyd"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6ziTZ3aTflj",
        "outputId": "1ae59943-bc7a-4c50-c477-e2c02adae351"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG59i7w5RCu_",
        "outputId": "cbeab60b-d802-4162-fa62-42f100677259"
      },
      "source": [
        "cd Text2App/training_RoBERTa/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Text2App/training_RoBERTa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaSAlz0SOjqs"
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1-5pNT7TVkYPesJvnmniuAT_Ru_gD_3D2\n",
        "# !gdown https://drive.google.com/uc?id=1FmqSYNx1pzqZ5r1YxWxTN6FpR7s0gTmT\n",
        "# !gdown https://drive.google.com/uc?id=1-4_Sx8GZBJA96cKQqytT4BZRkA8YPIb4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx5HeX1IZK17",
        "outputId": "af9f58d1-7dc0-44aa-b5d8-2d2155e73996"
      },
      "source": [
        "# For saving checkpoint to Google Drive while working on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yud9saHSVRi"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('../synthesized_data/nl_sar_train.csv')\n",
        "# dev = pd.read_csv('../synthesized_data/nl_sar_valid.csv')\n",
        "# test = pd.read_csv('../synthesized_data/nl_sar_test.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0AwB1I6Yhle"
      },
      "source": [
        "class MyTokenizer:\n",
        "  vocab_size = 0\n",
        "  vocab = []\n",
        "  id_to_token = {}\n",
        "  token_to_id = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab = list(set(\" \".join(list(train['SAR'])).split()))\n",
        "    self.vocab.sort()\n",
        "    self.vocab_size = len(self.token_to_id) \n",
        "    # Special tokens: <s><pad></s><unk>\n",
        "    self.add_token('<s>')\n",
        "    self.add_token('<pad>')\n",
        "    self.add_token('</s>')\n",
        "    self.add_token('<unk>')\n",
        "    for v in self.vocab:\n",
        "      self.add_token(v)\n",
        "    self.add_token('None')\n",
        "\n",
        "  def tokenize(self, s):\n",
        "    return s.split()\n",
        "\n",
        "  def add_token(self, s):\n",
        "    if s not in self.token_to_id:\n",
        "      self.id_to_token[self.vocab_size] = s\n",
        "      self.token_to_id[s] = self.vocab_size\n",
        "      self.vocab_size+=1\n",
        "\n",
        "  def convert_string_to_ids(self, s):\n",
        "    tokens = s.split()\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "      ids.append(self.token_to_id[token])\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\"\n",
        "    for id in ids:\n",
        "      text += self.id_to_token[id] + \" \"\n",
        "    return text[:-1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLOsuOtfPByQ"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbExmvP0-Aw"
      },
      "source": [
        "\n",
        "# Copyright (c) Microsoft Corporation. \n",
        "# Licensed under the MIT license.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "        Build Seqence-to-Sequence.\n",
        "        \n",
        "        Parameters:\n",
        "        * `encoder`- encoder of seq2seq model. e.g. roberta\n",
        "        * `decoder`- decoder of seq2seq model. e.g. transformer\n",
        "        * `config`- configuration of encoder model. \n",
        "        * `beam_size`- beam size for beam search. \n",
        "        * `max_length`- max length of target for beam search. \n",
        "        * `sos_id`- start of symbol ids in target for beam search.\n",
        "        * `eos_id`- end of symbol ids in target for beam search. \n",
        "    \"\"\"\n",
        "    def __init__(self, encoder,decoder,config,beam_size=None,max_length=None,sos_id=None,eos_id=None):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder=decoder\n",
        "        self.config=config\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, decoder_tokenizer.vocab_size, bias=False) #config.vocab_size\n",
        "        self.lsm = nn.LogSoftmax(dim=-1)\n",
        "        # self.tie_weights()\n",
        "        \n",
        "        self.beam_size=beam_size\n",
        "        self.max_length=max_length\n",
        "        self.sos_id=sos_id\n",
        "        self.eos_id=eos_id\n",
        "        \n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
        "        \"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "                  \n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.lm_head,\n",
        "                                   self.encoder.embeddings.word_embeddings)        \n",
        "        \n",
        "    def forward(self, source_ids=None,source_mask=None,target_ids=None,target_mask=None,args=None):   \n",
        "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
        "        encoder_output = outputs[0].permute([1,0,2]).contiguous()\n",
        "        if target_ids is not None:  \n",
        "            attn_mask=-1e4 *(1-self.bias[:target_ids.shape[1],:target_ids.shape[1]])\n",
        "            tgt_embeddings = self.encoder.embeddings(target_ids).permute([1,0,2]).contiguous() ## MH: Problmatic\n",
        "            out = self.decoder(tgt_embeddings,encoder_output,tgt_mask=attn_mask,memory_key_padding_mask=(1-source_mask).bool())\n",
        "            hidden_states = torch.tanh(self.dense(out)).permute([1,0,2]).contiguous()\n",
        "            lm_logits = self.lm_head(hidden_states)\n",
        "            # Shift so that tokens < n predict n\n",
        "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = target_ids[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
        "                            shift_labels.view(-1)[active_loss])\n",
        "\n",
        "            outputs = loss, loss*active_loss.sum(), active_loss.sum()\n",
        "            # print(\"Inside forward, outputs:\", outputs)\n",
        "            return outputs\n",
        "        else:\n",
        "            #Predict \n",
        "            preds=[]       \n",
        "            zero=torch.cuda.LongTensor(1).fill_(0)     \n",
        "            for i in range(source_ids.shape[0]):\n",
        "                context=encoder_output[:,i:i+1]\n",
        "                context_mask=source_mask[i:i+1,:]\n",
        "                beam = Beam(self.beam_size,self.sos_id,self.eos_id)\n",
        "                input_ids=beam.getCurrentState()\n",
        "                context=context.repeat(1, self.beam_size,1)\n",
        "                context_mask=context_mask.repeat(self.beam_size,1)\n",
        "                for _ in range(self.max_length): \n",
        "                    if beam.done():\n",
        "                        break\n",
        "                    attn_mask=-1e4 *(1-self.bias[:input_ids.shape[1],:input_ids.shape[1]])\n",
        "                    tgt_embeddings = self.encoder.embeddings(input_ids).permute([1,0,2]).contiguous()\n",
        "                    out = self.decoder(tgt_embeddings,context,tgt_mask=attn_mask,memory_key_padding_mask=(1-context_mask).bool())\n",
        "                    out = torch.tanh(self.dense(out))\n",
        "                    hidden_states=out.permute([1,0,2]).contiguous()[:,-1,:]\n",
        "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
        "                    beam.advance(out)\n",
        "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
        "                    input_ids=torch.cat((input_ids,beam.getCurrentState()),-1)\n",
        "                hyp= beam.getHyp(beam.getFinal())\n",
        "                pred=beam.buildTargetTokens(hyp)[:self.beam_size]\n",
        "                pred=[torch.cat([x.view(-1) for x in p]+[zero]*(self.max_length-len(p))).view(1,-1) for p in pred]\n",
        "                preds.append(torch.cat(pred,0).unsqueeze(0))\n",
        "                \n",
        "            preds=torch.cat(preds,0)\n",
        "            # print(\"Inside Forward, preds: \", preds)\n",
        "            return preds   \n",
        "        \n",
        "        \n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, size,sos,eos):\n",
        "        self.size = size\n",
        "        self.tt = torch.cuda\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = self.tt.FloatTensor(size).zero_()\n",
        "        # The backpointers at each time-step.\n",
        "        self.prevKs = []\n",
        "        # The outputs at each time-step.\n",
        "        self.nextYs = [self.tt.LongTensor(size)\n",
        "                       .fill_(0)]\n",
        "        self.nextYs[0][0] = sos\n",
        "        # Has EOS topped the beam yet.\n",
        "        self._eos = eos\n",
        "        self.eosTop = False\n",
        "        # Time and k pair for finished.\n",
        "        self.finished = []\n",
        "\n",
        "    def getCurrentState(self):\n",
        "        \"Get the outputs for the current timestep.\"\n",
        "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
        "        return batch\n",
        "\n",
        "    def getCurrentOrigin(self):\n",
        "        \"Get the backpointers for the current timestep.\"\n",
        "        return self.prevKs[-1]\n",
        "\n",
        "    def advance(self, wordLk):\n",
        "        \"\"\"\n",
        "        Given prob over words for every last beam `wordLk` and attention\n",
        "        `attnOut`: Compute and update the beam search.\n",
        "        Parameters:\n",
        "        * `wordLk`- probs of advancing from the last step (K x words)\n",
        "        * `attnOut`- attention at the last step\n",
        "        Returns: True if beam search is complete.\n",
        "        \"\"\"\n",
        "        numWords = wordLk.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prevKs) > 0:\n",
        "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
        "\n",
        "            # Don't let EOS have children.\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] == self._eos:\n",
        "                    beamLk[i] = -1e20\n",
        "        else:\n",
        "            beamLk = wordLk[0]\n",
        "        flatBeamLk = beamLk.view(-1)\n",
        "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
        "\n",
        "        self.scores = bestScores\n",
        "\n",
        "        # bestScoresId is flattened beam x word array, so calculate which\n",
        "        # word and beam each score came from\n",
        "        prevK = bestScoresId // numWords\n",
        "        self.prevKs.append(prevK)\n",
        "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
        "\n",
        "\n",
        "        for i in range(self.nextYs[-1].size(0)):\n",
        "            if self.nextYs[-1][i] == self._eos:\n",
        "                s = self.scores[i]\n",
        "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
        "\n",
        "        # End condition is when top-of-beam is EOS and no global score.\n",
        "        if self.nextYs[-1][0] == self._eos:\n",
        "            self.eosTop = True\n",
        "\n",
        "    def done(self):\n",
        "        return self.eosTop and len(self.finished) >=self.size\n",
        "\n",
        "    def getFinal(self):\n",
        "        if len(self.finished) == 0:\n",
        "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
        "        self.finished.sort(key=lambda a: -a[0])\n",
        "        if len(self.finished) != self.size:\n",
        "            unfinished=[]\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] != self._eos:\n",
        "                    s = self.scores[i]\n",
        "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
        "            unfinished.sort(key=lambda a: -a[0])\n",
        "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
        "        return self.finished[:self.size]\n",
        "\n",
        "    def getHyp(self, beam_res):\n",
        "        \"\"\"\n",
        "        Walk back to construct the full hypothesis.\n",
        "        \"\"\"\n",
        "        hyps=[]\n",
        "        for _,timestep, k in beam_res:\n",
        "            hyp = []\n",
        "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
        "                hyp.append(self.nextYs[j+1][k])\n",
        "                k = self.prevKs[j][k]\n",
        "            hyps.append(hyp[::-1])\n",
        "        return hyps\n",
        "    \n",
        "    def buildTargetTokens(self, preds):\n",
        "        sentence=[]\n",
        "        for pred in preds:\n",
        "            tokens = []\n",
        "            for tok in pred:\n",
        "                if tok==self._eos:\n",
        "                    break\n",
        "                tokens.append(tok)\n",
        "            sentence.append(tokens)\n",
        "        return sentence\n",
        "        \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrH5YUx6IvWz"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "import os\n",
        "import sys\n",
        "import bleu\n",
        "import pickle\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "# import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import open\n",
        "from itertools import cycle\n",
        "import torch.nn as nn\n",
        "# from model import Seq2Seq\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
        "                          RobertaConfig, RobertaModel, RobertaTokenizer)\n",
        "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer)}\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Example(object):\n",
        "    \"\"\"A single training/test example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 idx,\n",
        "                 source,\n",
        "                 target,\n",
        "                 ):\n",
        "        self.idx = idx\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "\n",
        "def read_examples(filename):\n",
        "    \"\"\"Read examples from filename.\"\"\"\n",
        "    examples=[]\n",
        "\n",
        "    ## open DF, convert to json, iterate one by one\n",
        "    # with open(filename,encoding=\"utf-8\") as f:\n",
        "    #     for idx, line in enumerate(f):\n",
        "    #         line=line.strip()\n",
        "    #         js=json.loads(line)\n",
        "    #         if 'idx' not in js:\n",
        "    #             js['idx']=idx\n",
        "    #         sar=' '.join(js['SAR']).replace('\\n',' ')\n",
        "    #         sar=' '.join(code.strip().split())\n",
        "    #         nl=' '.join(js['NL']).replace('\\n','')\n",
        "    #         nl=' '.join(nl.strip().split())  \n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    data_list = list(df.T.to_dict().values())\n",
        "    for idx, data in enumerate(data_list):\n",
        "      nl = data['NL']\n",
        "      sar = data['SAR']\n",
        "      examples.append(\n",
        "          Example(\n",
        "                  idx = idx,\n",
        "                  source=nl,\n",
        "                  target=sar,\n",
        "                  ) \n",
        "      )\n",
        "    return examples\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single training/test features for a example.\"\"\"\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.source_ids = source_ids\n",
        "        self.target_ids = target_ids\n",
        "        self.source_mask = source_mask\n",
        "        self.target_mask = target_mask       \n",
        "        \n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args,stage=None): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        #source\n",
        "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
        "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "        source_mask = [1] * (len(source_tokens))\n",
        "        padding_length = args.max_source_length - len(source_ids)\n",
        "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        source_mask+=[0]*padding_length\n",
        "\n",
        "        #target\n",
        "        if stage==\"test\":\n",
        "            target_tokens = ['None'] #tokenizer.tokenize(\"None\")\n",
        "        else:\n",
        "            # target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
        "            target_tokens = example.target.split()[:args.max_target_length-2]\n",
        "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "        # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "        target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "        target_mask = [1] *len(target_ids)\n",
        "        padding_length = args.max_target_length - len(target_ids)\n",
        "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "        target_mask+=[0]*padding_length   \n",
        "\n",
        "        if example_index < 5:\n",
        "            if stage=='train':\n",
        "                print(\"*** Example ***\")\n",
        "                print(\"idx: {}\".format(example.idx))\n",
        "\n",
        "                print(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
        "                print(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
        "                print(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
        "                \n",
        "                print(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
        "                print(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
        "                print(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
        "       \n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                 example_index,\n",
        "                 source_ids,\n",
        "                 target_ids,\n",
        "                 source_mask,\n",
        "                 target_mask,\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "def main():\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed)\n",
        "    # make dir if output_dir not exist\n",
        "    if os.path.exists(args.output_dir) is False:\n",
        "        os.makedirs(args.output_dir)\n",
        "        \n",
        "## model was here\n",
        "\n",
        "    if args.do_train:\n",
        "        print(\"Inside TRAIN\")\n",
        "        # Prepare training data loader\n",
        "        train_examples = read_examples(args.train_filename)\n",
        "        train_features = convert_examples_to_features(train_examples, tokenizer,args,stage='train') # MH: 2 tokenizers\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in train_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in train_features], dtype=torch.long)\n",
        "        all_target_ids = torch.tensor([f.target_ids for f in train_features], dtype=torch.long)\n",
        "        all_target_mask = torch.tensor([f.target_mask for f in train_features], dtype=torch.long)    \n",
        "        train_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)\n",
        "        \n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size//args.gradient_accumulation_steps)\n",
        "\n",
        "        num_train_optimization_steps =  args.train_steps\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': args.weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=int(t_total*0.1),\n",
        "                                                    num_training_steps=t_total)\n",
        "    \n",
        "        #Start training\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"  Num examples = %d\", len(train_examples))\n",
        "        print(\"  Batch size = %d\", args.train_batch_size)\n",
        "        print(\"  Num epoch = %d\", args.num_train_epochs)\n",
        "        \n",
        "\n",
        "        model.train()\n",
        "        dev_dataset={}\n",
        "        nb_tr_examples, nb_tr_steps,tr_loss,global_step,best_bleu,best_loss = 0, 0,0,0,0,1e6 \n",
        "        for epoch in range(args.num_train_epochs):\n",
        "            bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
        "            for batch in bar:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                source_ids,source_mask,target_ids,target_mask = batch\n",
        "                loss,_,_ = model(source_ids=source_ids,source_mask=source_mask,target_ids=target_ids,target_mask=target_mask)\n",
        "\n",
        "                if args.n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "                tr_loss += loss.item()\n",
        "                train_loss=round(tr_loss*args.gradient_accumulation_steps/(nb_tr_steps+1),4)\n",
        "                bar.set_description(\"epoch {} loss {}\".format(epoch,train_loss))\n",
        "                nb_tr_examples += source_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                loss.backward()\n",
        "\n",
        "                if (nb_tr_steps + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    #Update parameters\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    scheduler.step()\n",
        "                    global_step += 1\n",
        "\n",
        "            if args.do_eval:\n",
        "                print(\"Inside EVAL\")\n",
        "                #Eval model with dev dataset\n",
        "                tr_loss = 0\n",
        "                nb_tr_examples, nb_tr_steps = 0, 0                     \n",
        "                eval_flag=False    \n",
        "                if 'dev_loss' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_loss']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='dev')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)\n",
        "                    all_target_ids = torch.tensor([f.target_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_target_mask = torch.tensor([f.target_mask for f in eval_features], dtype=torch.long)      \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask,all_target_ids,all_target_mask)   \n",
        "                    dev_dataset['dev_loss']=eval_examples,eval_data\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                print(\"\\n***** Running evaluation *****\")\n",
        "                print(\"  Num examples = %d\", len(eval_examples))\n",
        "                print(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "                #Start Evaling model\n",
        "                model.eval()\n",
        "                eval_loss,tokens_num = 0,0\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask,target_ids,target_mask = batch                  \n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _,loss,num = model(source_ids=source_ids,source_mask=source_mask,\n",
        "                                           target_ids=target_ids,target_mask=target_mask)     \n",
        "                    eval_loss += loss.sum().item()\n",
        "                    tokens_num += num.sum().item()\n",
        "                #Pring loss of dev dataset    \n",
        "                model.train()\n",
        "                eval_loss = eval_loss / tokens_num\n",
        "                result = {'eval_ppl': round(np.exp(eval_loss),5),\n",
        "                          'global_step': global_step+1,\n",
        "                          'train_loss': round(train_loss,5)}\n",
        "                for key in sorted(result.keys()):\n",
        "                    print(\"  %s = %s\", key, str(result[key]))\n",
        "                print(\"  \"+\"*\"*20)   \n",
        "\n",
        "                #save last checkpoint\n",
        "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
        "                if not os.path.exists(last_output_dir):\n",
        "                    os.makedirs(last_output_dir)\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)                    \n",
        "                if eval_loss<best_loss:\n",
        "                    print(\"  Best ppl:%s\",round(np.exp(eval_loss),5))\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_loss=eval_loss\n",
        "                    # Save best checkpoint for best ppl\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)  \n",
        "\n",
        "\n",
        "                #Calculate bleu  \n",
        "                print(\"Calculating BLEU\")\n",
        "                if 'dev_bleu' in dev_dataset:\n",
        "                    eval_examples,eval_data=dev_dataset['dev_bleu']\n",
        "                else:\n",
        "                    eval_examples = read_examples(args.dev_filename)\n",
        "                    eval_examples = random.sample(eval_examples,min(1000,len(eval_examples)))\n",
        "                    eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "                    all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "                    all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "                    eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "                    dev_dataset['dev_bleu']=eval_examples,eval_data\n",
        "\n",
        "\n",
        "\n",
        "                eval_sampler = SequentialSampler(eval_data)\n",
        "                eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "                model.eval() \n",
        "                p=[]\n",
        "                for batch in eval_dataloader:\n",
        "                    batch = tuple(t.to(device) for t in batch)\n",
        "                    source_ids,source_mask= batch                  \n",
        "                    with torch.no_grad():\n",
        "                        preds = model(source_ids=source_ids,source_mask=source_mask)  \n",
        "                        for pred in preds:\n",
        "                            t=pred[0].cpu().numpy()\n",
        "                            t=list(t)\n",
        "                            if 0 in t:\n",
        "                                t=t[:t.index(0)]\n",
        "                            # text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
        "                            text = decoder_tokenizer.decode(t) # MH: decoder\n",
        "                            p.append(text)\n",
        "                model.train()\n",
        "                predictions=[]\n",
        "                with open(os.path.join(args.output_dir,\"dev.output\"),'w') as f, open(os.path.join(args.output_dir,\"dev.gold\"),'w') as f1:\n",
        "                    for ref,gold in zip(p,eval_examples):\n",
        "                        predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                        f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                        f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "                (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, \"dev.gold\")) \n",
        "                dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "                print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "                print(\"  \"+\"*\"*20)    \n",
        "                if dev_bleu>best_bleu:\n",
        "                    print(\"  Best bleu:%s\",dev_bleu)\n",
        "                    print(\"  \"+\"*\"*20)\n",
        "                    best_bleu=dev_bleu\n",
        "                    # Save best checkpoint for best bleu\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "               \n",
        "    if args.do_test:\n",
        "        # print(\"Inside TEST\")\n",
        "        # files=[]\n",
        "        # if args.dev_filename is not None:\n",
        "        #     files.append(args.dev_filename)\n",
        "        # if args.test_filename is not None:\n",
        "        #     files.append(args.test_filename)\n",
        "        # for idx,file in enumerate(files):   \n",
        "        idx = 0\n",
        "        file = args.test_filename # Change it to test file later\n",
        "        print(\"Test file: {}\".format(file))\n",
        "        eval_examples = read_examples(file)\n",
        "        eval_features = convert_examples_to_features(eval_examples, tokenizer, args,stage='test')\n",
        "        all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long)\n",
        "        all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long)    \n",
        "        eval_data = TensorDataset(all_source_ids,all_source_mask)   \n",
        "\n",
        "        # Calculate bleu\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        model.eval() \n",
        "        p=[]\n",
        "        for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            source_ids,source_mask= batch                  \n",
        "            with torch.no_grad():\n",
        "                preds = model(source_ids=source_ids,source_mask=source_mask) \n",
        "                for pred in preds:\n",
        "                    t=pred[0].cpu().numpy()\n",
        "                    t=list(t)\n",
        "                    if 0 in t:\n",
        "                        t=t[:t.index(0)]\n",
        "                    # text = tokenizer.decode(t,clean_up_tokenization_spaces=False) # MH: decoder\n",
        "                    text = decoder_tokenizer.decode(t)\n",
        "                    p.append(text)\n",
        "        model.train()\n",
        "        predictions=[]\n",
        "        with open(os.path.join(args.output_dir,\"test_{}.output\".format(str(idx))),'w') as f, open(os.path.join(args.output_dir,\"test_{}.gold\".format(str(idx))),'w') as f1:\n",
        "            for ref,gold in zip(p,eval_examples):\n",
        "                predictions.append(str(gold.idx)+'\\t'+ref)\n",
        "                f.write(str(gold.idx)+'\\t'+ref+'\\n')\n",
        "                f1.write(str(gold.idx)+'\\t'+gold.target+'\\n')     \n",
        "\n",
        "        (goldMap, predictionMap) = bleu.computeMaps(predictions, os.path.join(args.output_dir, \"test_{}.gold\".format(idx))) \n",
        "        dev_bleu=round(bleu.bleuFromMaps(goldMap, predictionMap)[0],2)\n",
        "        print(\"  %s = %s \"%(\"bleu-4\",str(dev_bleu)))\n",
        "        print(\"  \"+\"*\"*20)    \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffH7YvV6Oaxz"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34alizypPOH9"
      },
      "source": [
        "class Arguments:\n",
        "    pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaxEIuUM2uG"
      },
      "source": [
        "output_dir=\"model/\"\n",
        "data_dir = '../synthesized_data/'\n",
        "train_file=data_dir+'nl_sar_train.csv'\n",
        "dev_file=data_dir+'nl_sar_valid.csv'\n",
        "test_file=data_dir+'nl_sar_test.csv'\n",
        "pretrained_model='microsoft/codebert-base'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpOUBhufLt-w"
      },
      "source": [
        "args = Arguments()\n",
        "\n",
        "## Required parameters\n",
        "args.model_type='roberta'\n",
        "args.model_name_or_path=pretrained_model\n",
        "args.output_dir=output_dir\n",
        "args.load_model_path=None #output_dir+\"/checkpoint-best-bleu/pytorch_model.bin\"\n",
        "## Other parameters\n",
        "args.train_filename=train_file\n",
        "args.dev_filename=dev_file\n",
        "args.test_filename=test_file\n",
        "args.config_name=\"\"\n",
        "args.tokenizer_name=\"\"\n",
        "args.gradient_accumulation_steps=1\n",
        "args.weight_decay=0.0\n",
        "args.adam_epsilon=1e-8\n",
        "args.max_grad_norm=1.0\n",
        "args.max_steps=-1\n",
        "args.eval_steps=-1\n",
        "args.train_steps=-1\n",
        "args.warmup_steps=0\n",
        "args.local_rank=-1\n",
        "args.seed=42\n",
        "\n",
        "args.no_cuda=False\n",
        "args.do_lower_case=True\n",
        "args.do_train=True\n",
        "args.do_eval=True\n",
        "args.do_test=False\n",
        "\n",
        "args.num_train_epochs=10\n",
        "args.train_batch_size=100\n",
        "args.eval_batch_size=100\n",
        "args.learning_rate=5e-5\n",
        "args.max_source_length=80\n",
        "args.max_target_length=50\n",
        "args.beam_size=5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSceeljfzY0I",
        "outputId": "73d42419-2edc-49ec-dd24-e672646e95e4"
      },
      "source": [
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1))\n",
        "args.device = device"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YVnN-E_zEcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "7cc39e0fbde34ba5a1123403dad1addc",
            "0378b7a748a8487fa88d707f0361dae7",
            "35726ef88a9e44c4a9ae3e3a4486ab6f",
            "e97a9174ba2941f58397b4c904f3a56c",
            "ada11a22845d46a2b8d2c4b2d897547f",
            "a9da3b3f3613416cb491daeaf4faca70",
            "0a2642302ad34ecea6a1e17b8007e124",
            "5584fa2849674d22af1d55801d124675",
            "2fd7601f47a842ada775539fce2594bf",
            "4efc45da339a4e72a939b80a8ec3bb81",
            "cf6519034e6349b3b7553dd7c4389e2c",
            "20367294f4294edc937b79534b923a8c",
            "7983c2da71734663862fa92597a17f6b",
            "b0bec18ca9674822b75f8b2935959071",
            "57d8249f7822454d98ab4018cdad850f",
            "154b38827bd04bb097df5a24734fb86e",
            "6711a825a8754b0e8214123be210db12",
            "12893659767b4657bd3833872ada8f95",
            "cdd02fd616b94263861e50cdf82c6119",
            "75c53f42c79e4732b765c440b4046e98",
            "6bb57f46dba94105b232d658f805eccb",
            "9818bf784b81439391603a96d05b84d6",
            "a3bdaf5d29714a709bd7298cb9fdae48",
            "50756a8b37ab44dab0faa4a197465c29",
            "ec9c736619de47288bb25b3b59ad6f78",
            "cd73be9435a54dd3b860e666263569a6",
            "23a529bb4303446d8d054b7ca13c795a",
            "2cbe9b8240df4be3961443f09589cdd6",
            "5d1463aef32544febc58e0ccf69a8af7",
            "c67f539f22634bf1936f59705a61a417",
            "d6bba3f9de8344c5884248a8c75675ea",
            "6c7e1315c1c74884bfad2191bd3a90c0",
            "43e6ac5249644e0c894ec88f210129c7",
            "cc00f3d4f9894db2878927639503977e",
            "46a30bfbb5ee425aa6429d82624e3702",
            "a61d7a9f20fa497fab648c40084c012e",
            "aa002f87cb7f4cd09ae3062feb733688",
            "e2c2afb44fff4b8393f8eb3014498844",
            "ce8f5553ad794670979d24f392ebd3a9",
            "63f0062defe54bbd9089b98152fd1f41",
            "5133eec1fd52496ea913a7d2fa82cc42",
            "a8efcb940ef14db1bacb4aa2cc2ca923",
            "85bcce111b0e495a8338413081267d3d",
            "353b99d6ae354d0c97a0197475840a94",
            "55a3be037c5041d1ba8f1a5ac7e5442d",
            "844d1ed354fd44089f5fcb2062a584f9",
            "b0dcd14c184f4a36a8dd728b2aa1d0c1",
            "10be7dca34284199835ee54c778a6678"
          ]
        },
        "outputId": "2831b496-17c4-46af-df6d-e7d3a003a70f"
      },
      "source": [
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,do_lower_case=args.do_lower_case)\n",
        "\n",
        "decoder_tokenizer = MyTokenizer()\n",
        "\n",
        "#budild model\n",
        "encoder = model_class.from_pretrained(args.model_name_or_path,config=config)    \n",
        "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
        "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186505168 acquired on /root/.cache/huggingface/transformers/f5d03310b21357fca0146b028dd11e73204fc363aaf9d8a597441bf84b4bf7ce.c807bbe0e00c0455ed434f796bcc6b64b1b0a0d4f692f5095f3ea4abe955ed1e.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cc39e0fbde34ba5a1123403dad1addc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=498.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186505168 released on /root/.cache/huggingface/transformers/f5d03310b21357fca0146b028dd11e73204fc363aaf9d8a597441bf84b4bf7ce.c807bbe0e00c0455ed434f796bcc6b64b1b0a0d4f692f5095f3ea4abe955ed1e.lock\n",
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140074200514128 acquired on /root/.cache/huggingface/transformers/02cd3b4a22c34a00effa2ac201cd58c9e095c0d3e45b5dc3129943022c28ca4c.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fd7601f47a842ada775539fce2594bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140074200514128 released on /root/.cache/huggingface/transformers/02cd3b4a22c34a00effa2ac201cd58c9e095c0d3e45b5dc3129943022c28ca4c.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05.lock\n",
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140075583194192 acquired on /root/.cache/huggingface/transformers/c76adf801775a499b11324a2df6dbf146e811fd10a5b2fc5d0adddcbfd316cea.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6711a825a8754b0e8214123be210db12",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140075583194192 released on /root/.cache/huggingface/transformers/c76adf801775a499b11324a2df6dbf146e811fd10a5b2fc5d0adddcbfd316cea.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186674320 acquired on /root/.cache/huggingface/transformers/4da22fec5578ecec5fbe32b80d5a43c6fb66b0f6ad5e359979463f0722ac3b5f.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec9c736619de47288bb25b3b59ad6f78",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186674320 released on /root/.cache/huggingface/transformers/4da22fec5578ecec5fbe32b80d5a43c6fb66b0f6ad5e359979463f0722ac3b5f.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8.lock\n",
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186505040 acquired on /root/.cache/huggingface/transformers/dddd3977516a2f1e65e8e4b3293f63b07025e829b0a9fd3621cdd9e08c147a23.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43e6ac5249644e0c894ec88f210129c7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073186505040 released on /root/.cache/huggingface/transformers/dddd3977516a2f1e65e8e4b3293f63b07025e829b0a9fd3621cdd9e08c147a23.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:46 - INFO - filelock -   Lock 140073037166032 acquired on /root/.cache/huggingface/transformers/08477dcecf305af90229876aa01e4b0f3594dc8c638985a72277f39ea7d8d0c3.7fb14267817b1d26bb44a57cd5aa2fc003c25e87b75ef77e9c55c4804675b4cf.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5133eec1fd52496ea913a7d2fa82cc42",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=498627950.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "04/07/2021 10:59:56 - INFO - filelock -   Lock 140073037166032 released on /root/.cache/huggingface/transformers/08477dcecf305af90229876aa01e4b0f3594dc8c638985a72277f39ea7d8d0c3.7fb14267817b1d26bb44a57cd5aa2fc003c25e87b75ef77e9c55c4804675b4cf.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_SNN0D9EB8p"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
        "if args.load_model_path is not None:\n",
        "    print(\"reload model from {}\".format(args.load_model_path))\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    \n",
        "model.to(device)\n",
        "if args.local_rank != -1:\n",
        "    # Distributed training\n",
        "    try:\n",
        "        from apex.parallel import DistributedDataParallel as DDP\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "    model = DDP(model)\n",
        "elif args.n_gpu > 1:\n",
        "    # multi-gpu training\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8fS_vh0I0P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ad54d3-65e5-40f2-fe6d-80cc489b9596"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Arguments object at 0x7f65548633d0>\n",
            "Inside TRAIN\n",
            "*** Example ***\n",
            "idx: 0\n",
            "source_tokens: ['<s>', 'make', '_software', '_consisting', '_of', '_a', '_ball', '_,', '_a', '_switch', '_with', '_text', '_string', '0', '_,', '_and', '_a', '_pas', 'word', '_text', '_box', '_.', '_when', '_the', '_ball', '_is', '_reaches', '_edge', ',', '_set', '_ball', '_in', '_motion', '_.', '</s>']\n",
            "source_ids: 0 19746 2257 17402 9 10 1011 2156 10 5405 19 2788 6755 288 2156 8 10 6977 14742 2788 2233 479 77 5 1011 16 11541 3543 6 278 1011 11 4298 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<ball>', '<switch>', 'string0', '</switch>', '<passwordtextbox>', '</complist>', '<code>', '<ball1flung>', '<ball1>', '<motion>', '</ball1>', '</ball1flung>', '</code>', '</s>']\n",
            "target_ids: 0 50 40 78 103 29 64 14 48 38 37 62 5 6 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 1\n",
            "source_tokens: ['<s>', 'mobile', '_application', '_that', '_has', '_a', '_switch', '_,', '_a', '_circle', '_,', '_a', '_password', '_field', '_,', '_a', '_password', '_box', '_,', '_a', '_switch', '_,', '_an', '_acceler', 'ometer', '_,', '_and', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_.', '_when', '_the', '_initial', '_switch', '_is', '_pressed', ',', '_set', '_the', '_radius', '_to', '_number', '0', '_.', '_when', '_the', '_ball', '_is', '_touches', '_edge', ',', '_pause', '_video', '_player', '_.', '</s>']\n",
            "source_ids: 0 25254 2502 14 34 10 5405 2156 10 7922 2156 10 14844 882 2156 10 14844 2233 2156 10 5405 2156 41 27416 12687 2156 8 10 569 869 19 1300 6755 288 479 77 5 2557 5405 16 11224 6 278 5 29943 7 346 288 479 77 5 1011 16 12325 3543 6 13787 569 869 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<switch>', 'switch1', '</switch>', '<ball>', '<passwordtextbox>', '<passwordtextbox>', '<switch>', 'switch2', '</switch>', '<accelerometer>', '<video_player>', 'string0', '</video_player>', '</complist>', '<code>', '<switch1flipped>', '<ball1>', '<radius>', 'number0', '</radius>', '</ball1>', '</switch1flipped>', '<ball1reach_edge>', '<video_player1>', '<stop>', '</video_player1>', '</ball1reach_edge>', '</code>', '</s>']\n",
            "target_ids: 0 50 78 111 29 40 64 64 78 112 29 36 92 103 34 14 48 75 37 70 100 24 5 26 39 89 74 31 7 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 2\n",
            "source_tokens: ['<s>', 'an', '_app', '_consisting', '_of', '_a', '_text', '2', 'speech', '_,', '_a', '_video', '_with', '_source', '_string', '0', '_,', '_a', '_circle', '_,', '_a', '_video', '_with', '_source', '_string', '1', '_,', '_a', '_switch', '_named', '_string', '2', '_,', '_a', '_label', '_,', '_and', '_a', '_video', '_with', '_source', '_string', '3', '_.', '_if', '_the', '_circle', '_is', '_thrown', ',', '_set', '_label', '_text', '_to', '_string', '4', '_.', '_when', '_the', '_switch', '_is', '_flip', 'ed', ',', '_play', '_the', '_video', '_.', '</s>']\n",
            "source_ids: 0 260 1553 17402 9 10 2788 176 40511 2156 10 569 19 1300 6755 288 2156 10 7922 2156 10 569 19 1300 6755 134 2156 10 5405 1440 6755 176 2156 10 6929 2156 8 10 569 19 1300 6755 246 479 114 5 7922 16 5629 6 278 6929 2788 7 6755 306 479 77 5 5405 16 11113 196 6 310 5 569 479 2 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<text2speech>', '<video_player>', 'string0', '</video_player>', '<ball>', '<video_player>', 'string1', '</video_player>', '<switch>', 'string2', '</switch>', '<label>', 'label1', '</label>', '<video_player>', 'string3', '</video_player>', '</complist>', '<code>', '<ball1reach_edge>', '<label1>', 'string4', '</label1>', '</ball1reach_edge>', '<switch1flipped>', '<video_player1>', '<start>', '</video_player1>', '</switch1flipped>', '</code>', '</s>']\n",
            "target_ids: 0 50 80 92 103 34 40 92 104 34 78 105 29 59 97 19 92 106 34 14 48 39 56 107 16 7 75 89 73 31 26 12 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 3\n",
            "source_tokens: ['<s>', 'create', '_software', '_with', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_,', '_and', '_an', '_audio', '_with', '_source', '_string', '1', '_.', '</s>']\n",
            "source_ids: 0 32845 2257 19 10 569 869 19 1300 6755 288 2156 8 41 6086 19 1300 6755 134 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<video_player>', 'string0', '</video_player>', '<player>', 'string1', '</player>', '</complist>', '</s>']\n",
            "target_ids: 0 50 92 103 34 69 104 23 14 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "idx: 4\n",
            "source_tokens: ['<s>', 'make', '_mobile', '_application', '_containing', '_a', '_video', '_player', '_with', '_source', '_string', '0', '_.', '</s>']\n",
            "source_ids: 0 19746 1830 2502 8200 10 569 869 19 1300 6755 288 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "target_tokens: ['<s>', '<complist>', '<video_player>', 'string0', '</video_player>', '</complist>', '</s>']\n",
            "target_ids: 0 50 92 103 34 14 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "target_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = %d 40000\n",
            "  Batch size = %d 100\n",
            "  Num epoch = %d 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 1.8726: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:30<00:00,  1.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.2496\n",
            "  %s = %s global_step 401\n",
            "  %s = %s train_loss 1.8726\n",
            "  ********************\n",
            "  Best ppl:%s 1.2496\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 94.36 \n",
            "  ********************\n",
            "  Best bleu:%s 94.36\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 loss 0.1208: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:45<00:00,  1.91s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.0451\n",
            "  %s = %s global_step 801\n",
            "  %s = %s train_loss 0.1208\n",
            "  ********************\n",
            "  Best ppl:%s 1.0451\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 97.34 \n",
            "  ********************\n",
            "  Best bleu:%s 97.34\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 loss 0.0458: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:44<00:00,  1.91s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.03935\n",
            "  %s = %s global_step 1201\n",
            "  %s = %s train_loss 0.0458\n",
            "  ********************\n",
            "  Best ppl:%s 1.03935\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 97.38 \n",
            "  ********************\n",
            "  Best bleu:%s 97.38\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 loss 0.0359: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:45<00:00,  1.91s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.02999\n",
            "  %s = %s global_step 1601\n",
            "  %s = %s train_loss 0.0359\n",
            "  ********************\n",
            "  Best ppl:%s 1.02999\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 97.4 \n",
            "  ********************\n",
            "  Best bleu:%s 97.4\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 loss 0.0348: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:46<00:00,  1.92s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.02663\n",
            "  %s = %s global_step 2001\n",
            "  %s = %s train_loss 0.0348\n",
            "  ********************\n",
            "  Best ppl:%s 1.02663\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 97.53 \n",
            "  ********************\n",
            "  Best bleu:%s 97.53\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 loss 0.0245: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [12:46<00:00,  1.92s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Inside EVAL\n",
            "\n",
            "***** Running evaluation *****\n",
            "  Num examples = %d 5000\n",
            "  Batch size = %d 100\n",
            "  %s = %s eval_ppl 1.02337\n",
            "  %s = %s global_step 2401\n",
            "  %s = %s train_loss 0.0245\n",
            "  ********************\n",
            "  Best ppl:%s 1.02337\n",
            "  ********************\n",
            "Calculating BLEU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Total: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  bleu-4 = 97.62 \n",
            "  ********************\n",
            "  Best bleu:%s 97.62\n",
            "  ********************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 loss 0.0217:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 234/400 [07:29<05:19,  1.92s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptSjxB5Ye96"
      },
      "source": [
        "# If working on colab, save checkpoint to Google Drive\n",
        "\n",
        "!cp model/checkpoint-best-bleu/pytorch_model.bin '/gdrive/My Drive/text2app_models/CodeBERT/' # CodeBERT RoBERTa PointerNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIV6rQ0oOUPd"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V6r9YtBOPWG"
      },
      "source": [
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "# args.load_model_path=output_dir+\"/checkpoint-last/pytorch_model.bin\"\n",
        "args.load_model_path='/gdrive/My Drive/text2app_models/CodeBERT/pytorch_model.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDVxq3eJ4Xf"
      },
      "source": [
        "**Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa9ag27Dyms"
      },
      "source": [
        "model=Seq2Seq(encoder=encoder,decoder=decoder,config=config,\n",
        "              beam_size=args.beam_size,max_length=args.max_target_length,\n",
        "              sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
        "if args.load_model_path is not None:\n",
        "    print(\"reload model from {}\".format(args.load_model_path))\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "    \n",
        "model.to(device)\n",
        "if args.local_rank != -1:\n",
        "    # Distributed training\n",
        "    try:\n",
        "        from apex.parallel import DistributedDataParallel as DDP\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "    model = DDP(model)\n",
        "elif args.n_gpu > 1:\n",
        "    # multi-gpu training\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x15hh5QgBweO"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeHTIMPkQ3Y4"
      },
      "source": [
        "## Single NL prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpi_W0EdNvZ"
      },
      "source": [
        "def single_example_to_feature(example, tokenizer): # MH: make it encoder_tokenizer, decoder_tokenizer\n",
        "    features = []\n",
        "    source_tokens = tokenizer.tokenize(example)[:args.max_source_length-2]\n",
        "    source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
        "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
        "    source_mask = [1] * (len(source_tokens))\n",
        "    padding_length = args.max_source_length - len(source_ids)\n",
        "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    source_mask+=[0]*padding_length\n",
        " \n",
        "    target_tokens = ['None']\n",
        "    target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
        "    # target_ids = tokenizer.convert_tokens_to_ids(target_tokens) # MH: decoder\n",
        "    target_ids = decoder_tokenizer.convert_string_to_ids(' '.join(target_tokens))\n",
        "    target_mask = [1] *len(target_ids)\n",
        "    padding_length = args.max_target_length - len(target_ids)\n",
        "    target_ids+=[tokenizer.pad_token_id]*padding_length\n",
        "    target_mask+=[0]*padding_length   \n",
        "\n",
        "    features.append(\n",
        "        InputFeatures(\n",
        "              0,\n",
        "              source_ids,\n",
        "              target_ids,\n",
        "              source_mask,\n",
        "              target_mask,\n",
        "        )\n",
        "    )\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJPiYhLiIWgK"
      },
      "source": [
        "args.eval_batch_size = 1\n",
        "args.do_test=True\n",
        "args.do_train=False\n",
        "args.beam_size=1\n",
        "args.load_model_path=output_dir+\"/checkpoint-last/pytorch_model.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogRkbfcwfDph"
      },
      "source": [
        "model.eval() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldJhnZ92w3aP"
      },
      "source": [
        "def get_sar(eval_example):\n",
        "  eval_features = single_example_to_feature(eval_example, tokenizer)\n",
        "  all_source_ids = torch.tensor([f.source_ids for f in eval_features], dtype=torch.long).to(device)\n",
        "  all_source_mask = torch.tensor([f.source_mask for f in eval_features], dtype=torch.long).to(device)\n",
        "  preds = model(source_ids=all_source_ids, source_mask=all_source_mask)  \n",
        "  pred_list = list(preds[0][0].cpu().numpy())\n",
        "  predicted_text = decoder_tokenizer.decode(pred_list[:pred_list.index(0)])\n",
        "  return predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "l40SKW1237Wr",
        "outputId": "c3b0804c-72b9-420b-eadb-4d54638b3166"
      },
      "source": [
        "eval_example = 'make software with a timepicker , a ball , a switch , and a video player with source string0 . if the ball is cornered, set the speed to number0 .'\n",
        "\n",
        "get_sar(eval_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start_comp> <timepicker> <ball> <switch> switch1 </switch> <video_player> string0 </video_player> <end_comp> <code> <ball1flung> <ball1> <speed> number0 </speed> </ball1> </ball1flung> </code>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "b3EpBuayf0dp",
        "outputId": "72b6ec70-01bc-413b-f1a1-35fa313db685"
      },
      "source": [
        "get_sar(\"make an app with an accelerometer and a music player . when accelerometer is shaken play music\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start_comp> <accelerometer> <player> random_player_source </player> <end_comp> <code> <accelerometer1shaken> <player1> <play> </player1> </accelerometer1shaken> </code>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCkz3n0gFj3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNsWW7HQNvs"
      },
      "source": [
        "## Calculate BLEU-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNOFrVjDQQbr"
      },
      "source": [
        "!python evaluator.py model/test_0.gold < model/test_0.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPK1mbrRtmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}